{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/noemicj/dataset-uci-heart-disease?scriptVersionId=115934783\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# **Classification using scikit-learn | Dataset UCI Heart Disease Dataset**","metadata":{}},{"cell_type":"code","source":"### Importing Files\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:04:44.17673Z","iopub.execute_input":"2023-01-09T19:04:44.177667Z","iopub.status.idle":"2023-01-09T19:04:44.186991Z","shell.execute_reply.started":"2023-01-09T19:04:44.17762Z","shell.execute_reply":"2023-01-09T19:04:44.185781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> As part of the final project of the course \"[Introduction to Data Science and scikit-learn in Python](https://www.coursera.org/learn/data-science-and-scikit-learn-in-python)\", we had to predict the presence of heart disease using [patient data](/kaggle/input/heart-disease-dataset-uci/HeartDiseaseTrain-Test.csv).","metadata":{}},{"cell_type":"markdown","source":"**Goal: presence/absence of heart disease based the following health-related features**\n\n* age: age in years\n* sex: (0 = female; 1 = male)\n* cp: chest pain type (0 = Typical angina; 1 = Atypical angina; 2 = Non-anginal pain; 3 = Asymptomatic)\n* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n* chol: serum cholestoral in mg/dl\n* fbs: (fasting blood sugar > 120 mg/dl) (0 = false; 1 = true)\n* restecg: resting electrocardiographic results (0 = Normal; 1 = ST-T Wave Abnormality; 2 = Left Ventricular Hypertrophy)\n* thalach: maximum heart rate achieved \n* exang: exercise induced angina (0 = no; 1 = yes)\n* oldpeak: ST depression induced by exercise relative to rest \n* slope: the slope of the peak exercise ST segment (0 = Upsloping; 1 = Flat; 2 = Downsloping)\n* ca: number of major vessels (0-3) colored by flourosopy\n* thal: A blood disorder called 'Thalassemia' (0 = Normal; 1 = Fixed Defect; 2 = Reversable Defect)\n* condition: have disease or not (0=no; 1=yes)","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv\", index_col=0)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:08:23.935436Z","iopub.execute_input":"2023-01-09T19:08:23.936039Z","iopub.status.idle":"2023-01-09T19:08:23.958159Z","shell.execute_reply.started":"2023-01-09T19:08:23.936004Z","shell.execute_reply":"2023-01-09T19:08:23.956913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 Data Preprocessing Techniques (50 pts)\n'''\na) Use one-hot encoding to transform the 'thal' feature into two columns called 'is_normal', 'is_fixed', \nand 'is_reversible'. (15 pts). Be sure to drop the 'thal' column afterwards.\nHint: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n'''","metadata":{}},{"cell_type":"code","source":"data['thal'].replace({0: 'normal', 1: 'fixed', 2: 'reversible'}, inplace = True)\ndata = pd.get_dummies(data, columns=[\"thal\"], prefix=[\"is\"])","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:08:30.606087Z","iopub.execute_input":"2023-01-09T19:08:30.60681Z","iopub.status.idle":"2023-01-09T19:08:30.761908Z","shell.execute_reply.started":"2023-01-09T19:08:30.606707Z","shell.execute_reply":"2023-01-09T19:08:30.759439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"b) Use min-max normalization to resacle all the features between 0 and 1 (15 pts). Make sure that data remains in the same\ndataframe format.\nHint: Use https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata = pd.DataFrame(scaler.fit_transform(data.values), columns=data.columns, index=data.index)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:08:38.490937Z","iopub.execute_input":"2023-01-09T19:08:38.491829Z","iopub.status.idle":"2023-01-09T19:08:38.518254Z","shell.execute_reply.started":"2023-01-09T19:08:38.491785Z","shell.execute_reply":"2023-01-09T19:08:38.517165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"c) Split the data into a train, test set using a 75/25 split. Use a random state of 42 for grading purposes (20 pts).\nHint: Use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html","metadata":{}},{"cell_type":"code","source":"X = data.drop(columns = 'condition')\ny = data['condition']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(X_train.shape, y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:15:14.590892Z","iopub.execute_input":"2023-01-09T19:15:14.591672Z","iopub.status.idle":"2023-01-09T19:15:14.602383Z","shell.execute_reply.started":"2023-01-09T19:15:14.59162Z","shell.execute_reply":"2023-01-09T19:15:14.600912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2: Fitting Model and Analyzing Results (50 pts)","metadata":{}},{"cell_type":"markdown","source":"a) Fit a logisitic regression classifier on the data. Save the model in a varaible called 'clf'. Use a random state of 42.\nUse the following paramters: penalty:'l2', solver:'liblinear', C:0.1. 15 pts.","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression(penalty='l2', solver='liblinear', C=0.1, random_state = 42)\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:09:04.167958Z","iopub.execute_input":"2023-01-09T19:09:04.168958Z","iopub.status.idle":"2023-01-09T19:09:04.183247Z","shell.execute_reply.started":"2023-01-09T19:09:04.168911Z","shell.execute_reply":"2023-01-09T19:09:04.181743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"b) Generate 0/1 predictions on the test set and store them in a varaible called 'pred'. \nGenerate probability predictions on the test set and store them in a variable called 'scores'.\n10 pts","metadata":{}},{"cell_type":"code","source":"pred = clf.predict(X_test)\nscores = clf.predict_proba(X_test)[:,1]\nprint('Accuracy: ', accuracy_score(y_test, pred))\nprint('AUROC: ', roc_auc_score(y_test, scores))\nprint(classification_report(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:15:55.477021Z","iopub.execute_input":"2023-01-09T19:15:55.477423Z","iopub.status.idle":"2023-01-09T19:15:55.497695Z","shell.execute_reply.started":"2023-01-09T19:15:55.477394Z","shell.execute_reply":"2023-01-09T19:15:55.496509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"c) Fill in this function to find and return the root mean sqaured error between the predicted and actual values.\nHint: Use his formula for the rsme: https://sciencing.com/calculate-mean-deviation-7152540.html.\n10 pts","metadata":{}},{"cell_type":"code","source":"def rsme(predictions, actuals):\n    from sklearn.metrics import mean_squared_error\n    return mean_squared_error(actuals, predictions, squared=False)\nprint('RSME: ', rsme(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:17:11.702844Z","iopub.execute_input":"2023-01-09T19:17:11.703241Z","iopub.status.idle":"2023-01-09T19:17:11.710198Z","shell.execute_reply.started":"2023-01-09T19:17:11.703211Z","shell.execute_reply":"2023-01-09T19:17:11.709243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"d) Try using a random forest classifier to fit the data instead. Use the default parameters and a random state of 42.\nSave the fitted model into a varaible called 'rf'. Generate the 'pred' and 'scores' in a similar way to part b.\nHint: Use https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n15 pts","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\npred = rf.predict(X_test)\nscores = rf.predict_proba(X_test)[:,1]\n\nprint('Accuracy: ', accuracy_score(y_test, pred))\nprint('AUROC: ', roc_auc_score(y_test, scores))\nprint(classification_report(y_test, pred))  \nprint('RSME: ', rsme(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T19:18:41.07943Z","iopub.execute_input":"2023-01-09T19:18:41.08036Z","iopub.status.idle":"2023-01-09T19:18:41.272383Z","shell.execute_reply.started":"2023-01-09T19:18:41.080317Z","shell.execute_reply":"2023-01-09T19:18:41.27108Z"},"trusted":true},"execution_count":null,"outputs":[]}]}